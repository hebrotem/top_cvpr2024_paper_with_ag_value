[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fhebrotem%2FTop_Cvpr2024_Paper_with_Ag_value&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=Page+visitors&edge_flat=false)](https://hits.seeyoufarm.com)
<!-- ![Hello badge](https://visitor-badge.laobi.icu/badge?page_id=hebrotem.Top_Cvpr2024_Paper_with_Ag_value)

<div align="center">
  <h1 align="center">Top CVPR 2024 Papers with Agriculture Application Values </h1>
  <a href="https://github.com/SkalskiP/top-cvpr-2023-papers">2023</a>
</div>
-->
<br>

<!--
<div align="center">
  <img width="600" src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/347853f9-9e93-4ca0-858b-a7c3f6bba073" alt="vancouver">
</div>
-->

## üëã hello
Computer Vision and Pattern Recognition (CVPR) is a major conference, and in 2024, it received 11,532 paper submissions, 
out of which 2,719 were accepted. I've created this repository to help you find the best of the best from CVPR publications. 
If the paper you seek isn't in my curated list, feel free to check out the full [list](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers) of accepted papers.
.

# „ÄêCVPR 2024 Papers Extendable to Agriculture„Äë

<details> <summary>List of Papers</summary>
    
  - [Image and video synthesis and generation](#Image-and-video-synthesis-and-generation)
  - [3D from multi-view and sensors](#3DfromMulti)
</details>

---
<a name="Image-and-video-synthesis-and-generation"></a>
### Image and video synthesis and generation
- <p align="left">
    <a href="https://arxiv.org/abs/2309.11497" title="FreeU: Free Lunch in Diffusion U-Net">
       <strong>FreeU: Free Lunch in Diffusion U-Net</strong>
    </a>
    <br/>
    Authors: Chenyang Si, Ziqi Huang, Yuming Jiang, Ziwei Liu
    <br/>
  <a href="https://arxiv.org/abs/2403.14602"><img src="https://img.shields.io/badge/arXiv-FreeU-b31b1b.svg" height=20.5></a> 
  <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Si_FreeU_Free_Lunch_in_Diffusion_U-Net_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/ChenyangSi/FreeU"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://chenyangsi.top/FreeU/"><img src="https://img.shields.io/badge/Project_Page-FreeU-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=eFmkJ_oEW5s"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5></a>   
    <br/>
    <strong>Likely application in agriculture:</strong> Data augmentation
</p>

- <p align="left">
    <a href="https://arxiv.org/abs/2402.19481" title="DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models">
       <strong>DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</strong>
    </a>
    <br/>
    Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Kai Li, Song Han
    <br/>
     <a href="https://arxiv.org/abs/2402.19481"><img src="https://img.shields.io/badge/arXiv-distriFusion-b31b1b.svg" height=20.5></a> <a href="href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/mit-han-lab/distrifuser"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://hanlab.mit.edu/projects/distrifusion"><img src="https://img.shields.io/badge/Project_Page-DistriFusion-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=EZX7srDDmW0"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5></a>
    <br/>
    <strong>Likely application in agriculture:</strong> Data augmentation
</p>

- <p align="left">
    <a href="https://arxiv.org/abs/2312.15540" title="Amodal Completion via Progressive Mixed Context Diffusion">
       <strong>Amodal Completion via Progressive Mixed Context Diffusion</strong>
    </a>
    <br/>
    Katherine Xu, Lingzhi Zhang, Jianbo Shi
    <br/>
    <a href="https://arxiv.org/abs/2312.15540"><img src="https://img.shields.io/badge/arXiv-Amodal-b31b1b.svg" height=20.5></a> <a href="href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Amodal_Completion_via_Progressive_Mixed_Context_Diffusion_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/k8xu/amodal"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://k8xu.github.io/amodal/"><img src="https://img.shields.io/badge/Project_Page-Amodal-blue' alt='Project Page"></a> <!-- <a href="https://www.youtube.com/watch?v=EZX7srDDmW0"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5></a> -->
    <br/>
    <strong>Likely application in agriculture:</strong> Occlusion-aware amodal Completion
</p>

- <p align="left">
    <a href="https://arxiv.org/abs/2405.20324" title="Don't Drop Your Samples! Coherence-Aware Training Benefits Conditional Diffusion">
       <strong>Don't Drop Your Samples! Coherence-Aware Training Benefits Conditional Diffusion</strong>
    </a>
    <br/>
    Nicolas Dufour, Victor Besnier, Vicky Kalogeiton, David Picard
    <br/>
    <a href="https://arxiv.org/abs/2405.20324"><img src="https://img.shields.io/badge/arXiv-Don't Drop-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Dufour_Dont_Drop_Your_Samples_Coherence-Aware_Training_Benefits_Conditional_Diffusion_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/nicolas-dufour/CAD"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://nicolas-dufour.github.io/cad"><img src="https://img.shields.io/badge/Project_Page-Don't_Drop-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=4Tu-x2-Zcxs"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5></a>
    <br/>
    <strong>Likely application in agriculture:</strong> Deep data augmentation
</p>

- <p align="left">
    <a href="https://arxiv.org/abs/2312.06038" title="Correcting Diffusion Generation through Resampling">
       <strong>Correcting Diffusion Generation through Resampling</strong>
    </a>
    <br/>
    Yujian Liu, Yang Zhang, Tommi Jaakkola, Shiyu Chang
    <br/>
     <a href="https://arxiv.org/abs/2312.06038"><img src="https://img.shields.io/badge/arXiv-Correcting-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Correcting_Diffusion_Generation_through_Resampling_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/UCSB-NLP-Chang/diffusion_resampling"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://nicolas-dufour.github.io/cad"><img src="https://img.shields.io/badge/Project_Page-Correcting-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=4Tu-x2-Zcxs"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5></a>
    <br/>
    <strong>Likely application in agriculture:</strong> Text-conditioned image generation
</p>

- <p align="left">
    <a href="https://arxiv.org/abs/2311.17002" title="Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following">
    <strong>Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following </strong>
    </a>
    <br/>
    Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, Jingren Zhou
    <br/>
    <a href="https://arxiv.org/abs/2311.17002"><img src="https://img.shields.io/badge/arXiv-Ranni-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Feng_Ranni_Taming_Text-to-Image_Diffusion_for_Accurate_Instruction_Following_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/ali-vilab/Ranni"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href='https://ranni-t2i.github.io/Ranni/'><img src="https://img.shields.io/badge/Project_Page-Ranni-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=1IIat83Atjk"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5> </a> 
    <br/>
    <strong>Likely application in agriculture:</strong> Text-to-Image
</p>

<!--- - <p align="left">
    <a href="https://arxiv.org/abs/2312.02149" title="Don't Drop Your Samples! Coherence-Aware Training Benefits Conditional Diffusion">
       <strong>Generative Powers of Ten</strong>
    </a>
    <br/>
    Xiaojuan Wang, Janne Kontkanen, Brian Curless, Steve Seitz, Ira Kemelmacher, Ben Mildenhall, Pratul Srinivasan, Dor Verbin, Aleksander Holynski
    <br/>
    <a href="https://arxiv.org/abs/2312.02149"><img src="https://img.shields.io/badge/arXiv-Gen Ten-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Generative_Powers_of_Ten_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://powers-of-10.github.io/"><img src="https://img.shields.io/badge/Project_Page-Generative-blue' alt='Project Page"></a>  <a href="https://www.youtube.com/watch?v=0fKBhvDjuy0"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5></a>
    <br/>
    <strong>Likely application in agriculture:</strong> Text-to-image
</p>

- <p align="left">
    <a href="https://arxiv.org/abs/2406.07480" title="Image Neural Field Diffusion Models">
       <strong>Image Neural Field Diffusion Models</strong>
    </a>
    <br/>
    Yinbo Chen, Oliver Wang, Richard Zhang, Eli Shechtman, Xiaolong Wang, Michael Gharbi
    <br/>
    <a href="https://arxiv.org/abs/2406.07480"><img src="https://img.shields.io/badge/arXiv-Image Neural-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Image_Neural_Field_Diffusion_Models_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://yinboc.github.io/infd/"><img src="https://img.shields.io/badge/Project_Page-Generative-blue' alt='Project Page"></a>
    <br/>
    <strong>Likely application in agriculture:</strong> Deep data augmentation
</p>
-->
- <p align="left">
    <a href="https://arxiv.org/abs/2403.01852" title="PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis">
       <strong>PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis</strong>
    </a>
    <br/>
    Zhengyao Lv, Yuxiang Wei, Wangmeng Zuo2, Kwan-Yee K. Wong
    <br/>
    <a href="https://arxiv.org/abs/2403.01852"><img src="https://img.shields.io/badge/arXiv-PLACE-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lv_PLACE_Adaptive_Layout-Semantic_Fusion_for_Semantic_Image_Synthesis_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/cszy98/PLACE"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://cszy98.github.io/PLACE/"><img src="https://img.shields.io/badge/Project_Page-PLACE-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=47mMAmclPWw"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5></a>
    <br/>
    <strong>Likely application in agriculture:</strong> Deep Segmatic image generation
</p>

- <p align="left">
    <a href="https://arxiv.org/abs/2312.02150" title="Readout Guidance: Learning Control from Diffusion Features">
       <strong>Readout Guidance: Learning Control from Diffusion Features</strong>
    </a>
    <br/>
    Grace Luo, Trevor Darrell, Oliver Wang, Dan B Goldman, Aleksander Holynski
    <br/>
    <a href="https://arxiv.org/abs/2312.02150"><img src="https://img.shields.io/badge/arXiv-Readout-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_Readout_Guidance_Learning_Control_from_Diffusion_Features_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/google-research/readout_guidance"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://readout-guidance.github.io/"><img src="https://img.shields.io/badge/Project_Page-Readout-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=VoA5WLRN-Oo"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5></a>
    <br/>
    <strong>Likely application in agriculture:</strong> Deep Segmatic image generation
</p>

- <p align="left">
    <a href="https://arxiv.org/abs/2311.16503" title="TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models">
       <strong>TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models</strong>
    </a>
    <br/>
    Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, Xianglong Liu
  <br/>
    <a href="https://arxiv.org/abs/2311.16503"><img src="https://img.shields.io/badge/arXiv-TFMQ_DM-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_TFMQ-DM_Temporal_Feature_Maintenance_Quantization_for_Diffusion_Models_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/ModelTC/TFMQ-DM"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://modeltc.github.io/TFMQ-DM/"><img src="https://img.shields.io/badge/Project_Page-TFMQ_DM-blue' alt='Project Page"></a> 
    <br/>
    <strong>Likely application in agriculture:</strong> Edge-efficient image generation
</p>

- <p align="left">
    <a href="https://arxiv.org/abs/2404.04650" title="InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization">
    <strong>InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization</strong>
    </a>
    <br/>
    Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, Di Huang
  <br/>
    <a href="https://arxiv.org/abs/2404.04650"><img src="https://img.shields.io/badge/arXiv-InitNo-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_InitNO_Boosting_Text-to-Image_Diffusion_Models_via_Initial_Noise_Optimization_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/xiefan-guo/initno"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://xiefan-guo.github.io/initno/"><img src="https://img.shields.io/badge/Project_Page-InitNO-blue' alt='Project Page"></a> 
    <br/>
    <strong>Likely application in agriculture:</strong> Text-to-Image
</p>

<!--- <p align="left">
    <a href="https://arxiv.org/abs/2312.10835" title="Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models">
    <strong>Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models</strong>
    </a>
    <br/>
    Nikita Starodubcev, Dmitry Baranchuk, Artem Fedorov, Artem Babenko
    <br/>
    <a href="https://arxiv.org/abs/2312.10835"><img src="https://img.shields.io/badge/arXiv-InitNo-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Starodubcev_Your_Student_is_Better_Than_Expected_Adaptive_Teacher-Student_Collaboration_for_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/yandex-research/adaptive-diffusion"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://readout-guidance.github.io/"><img src="https://img.shields.io/badge/Project_Page-Generative-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=VoA5WLRN-Oo"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5></a>
    <br/>
    <strong>Likely application in agriculture:</strong> Deep Segmatic image generation
</p> -->


<a name="3DfromMulti"></a>
### 3D from multi-view and sensors

- <p align="left">
    <a href="https://arxiv.org/abs/2304.08069" title="DETRs Beat YOLOs on Real-time Object Detection">
    <strong>DETRs Beat YOLOs on Real-time Object Detection</strong>
    </a>
    <br/>
    Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, Jie Chen
    <br/>
    <a href="https://arxiv.org/abs/2304.08069"><img src="https://img.shields.io/badge/arXiv-DETRs_YOLOs-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/lyuwenyu/RT-DETR"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://zhao-yian.github.io/RTDETR/"><img src="https://img.shields.io/badge/Project_Page-DETRs_YOLOs-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=UOc0qMSX4Ac"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5> </a> 
    <br/>
    <strong>Likely application in agriculture:</strong> disease detection
</p>

- <p align="left">
    <a href="https://arxiv.org/abs/2312.01220" title="Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation">
       <strong>Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation</strong>
    </a>
    <br/>
    Zhipeng Du, Miaojing Shi, Jiankang Deng
    <br/>
    <a href="https://arxiv.org/abs/2312.01220"><img src="https://img.shields.io/badge/arXiv-DETRs_YOLOs-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Du_Boosting_Object_Detection_with_Zero-Shot_Day-Night_Domain_Adaptation_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/ZPDu/DAI-Net"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://zpdu.github.io/DAINet_page/"><img src="https://img.shields.io/badge/Project_Page-Boosting-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=X44b2lInZzk"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5> </a> 
    <br/>
    <strong>Likely application in agriculture:</strong> disease detection
</p>

- <p align="left">
    <a href="https://arxiv.org/abs/2404.04319" title="SpatialTracker: Tracking Any 2D Pixels in 3D Space">
       <strong>SpatialTracker: Tracking Any 2D Pixels in 3D Space</strong>
    </a>
    <br/>
    Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, Xiaowei Zhou
    <br/>
    <a href="https://arxiv.org/abs/2304.08069"><img src="https://img.shields.io/badge/arXiv-DETRs_YOLOs-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/lyuwenyu/RT-DETR"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://zhao-yian.github.io/RTDETR/"><img src="https://img.shields.io/badge/Project_Page-DETRs_YOLOs-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=UOc0qMSX4Ac"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5> </a> 
    <br/>
    <strong>Likely application in agriculture:</strong> disease detection
</p>


- <p align="left">
    <a href="https://arxiv.org/abs/2403.01807" title="ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models">
        <strong>ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models</strong>
    </a>
    <br/>
    Lukas H√∂llein, Alja≈æ Bo≈æiƒç, Norman M√ºller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollh√∂fer, Matthias Nie√üner
    <br/>
       <a href="https://arxiv.org/abs/2304.08069"><img src="https://img.shields.io/badge/arXiv-DETRs_YOLOs-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/lyuwenyu/RT-DETR"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://zhao-yian.github.io/RTDETR/"><img src="https://img.shields.io/badge/Project_Page-DETRs_YOLOs-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=UOc0qMSX4Ac"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5> </a> 
    <br/>
    <strong>Likely application in agriculture:</strong> disease detection
</p>


- <p align="left">
    <a href="https://arxiv.org/abs/2405.12979" title="OmniGlue: Generalizable Feature Matching with Foundation Model Guidance">
        <strong>OmniGlue: Generalizable Feature Matching with Foundation Model Guidance</strong>
    </a>
    <br/>
    Hanwen Jiang, Arjun Karpur, Bingyi Cao, Qixing Huang, Andre Araujo
    <br/>
    <a href="https://arxiv.org/abs/2304.08069"><img src="https://img.shields.io/badge/arXiv-DETRs_YOLOs-b31b1b.svg" height=20.5></a> 
    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.pdf"><img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24" height=20.5></a> <a href="https://github.com/lyuwenyu/RT-DETR"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white", height=20.5></a> <a href="https://zhao-yian.github.io/RTDETR/"><img src="https://img.shields.io/badge/Project_Page-DETRs_YOLOs-blue' alt='Project Page"></a> <a href="https://www.youtube.com/watch?v=UOc0qMSX4Ac"><img src="https://img.shields.io/static/v1?label=Youtube&message=Link&color=red" height=20.5> </a> 
    <br/>
    <strong>Likely application in agriculture:</strong> disease detection
</p>

### deep learning architectures and techniques

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30529.png?t=1717455193.7819567" title="Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/4aaf3f87-cc62-4fa3-af99-c8c1c83c0069" alt="Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/pdf/2311.06242" title="Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks">
        <strong>üî• Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</strong>
    </a>
    <br/>
    Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan
    <br/>
    [<a href="https://arxiv.org/pdf/2311.06242">paper</a>]  [<a href="https://youtu.be/cOlyA00K1ec">video</a>] [<a href="https://huggingface.co/spaces/gokaygokay/Florence-2">demo</a>] [<a href="https://youtu.be/cOlyA00K1ec">colab</a>]
    <br/>
    <strong>Topic:</strong> Deep learning architectures and techniques
    <br/>
    <strong>Session:</strong> Wed 19 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #102
</p>
<br/>
<br/>

### document analysis and understanding

<p align="left">
    <a href="https://arxiv.org/abs/2405.04408" title="DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks">
        <strong>DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks</strong>
    </a>
    <br/>
    Jiaxin Zhang, Dezhi Peng, Chongyu Liu, Peirong Zhang, Lianwen Jin
    <br/>
    [<a href="https://arxiv.org/abs/2405.04408">paper</a>] [<a href="https://github.com/ZZZHANG-jx/DocRes">code</a>]  [<a href="https://huggingface.co/spaces/qubvel-hf/documents-restoration">demo</a>] 
    <br/>
    <strong>Topic:</strong> Document analysis and understanding
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #101
</p>
<br/>

### efficient and scalable vision

<p align="left">
    <a href="https://arxiv.org/abs/2312.00863" title="EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything">
        <strong>üî• EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</strong>
    </a>
    <br/>
    Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra
    <br/>
    [<a href="https://arxiv.org/abs/2312.00863">paper</a>] [<a href="https://github.com/yformer/EfficientSAM">code</a>]  [<a href="https://huggingface.co/spaces/SkalskiP/EfficientSAM">demo</a>] 
    <br/>
    <strong>Topic:</strong> Efficient and scalable vision
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #144
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30022.png?t=1718402790.003817" title="MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30022.png?t=1718402790.003817" alt="MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2311.17049" title="MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training">
        <strong>MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training</strong>
    </a>
    <br/>
    Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel
    <br/>
    [<a href="https://arxiv.org/abs/2311.17049">paper</a>] [<a href="https://github.com/apple/ml-mobileclip">code</a>]  [<a href="https://huggingface.co/spaces/Xenova/webgpu-mobileclip">demo</a>] 
    <br/>
    <strong>Topic:</strong> Efficient and scalable vision
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #130
</p>
<br/>
<br/>

### explainable computer vision

<p align="left">
    <a href="https://arxiv.org/abs/2312.02974" title="Describing Differences in Image Sets with Natural Language">
        <strong>üî• Describing Differences in Image Sets with Natural Language</strong>
    </a>
    <br/>
    Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, Serena Yeung-Levy
    <br/>
    [<a href="https://arxiv.org/abs/2312.02974">paper</a>] [<a href="https://github.com/Understanding-Visual-Datasets/VisDiff">code</a>]   
    <br/>
    <strong>Topic:</strong> Explainable computer vision
    <br/>
    <strong>Session:</strong> Fri 21 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #115
</p>
<br/>

### image and video synthesis and generation

<p align="left">
    <a href="https://arxiv.org/abs/2311.16973" title="DemoFusion: Democratising High-Resolution Image Generation With No $$$">
        <strong>DemoFusion: Democratising High-Resolution Image Generation With No $$$</strong>
    </a>
    <br/>
    Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma
    <br/>
    [<a href="https://arxiv.org/abs/2311.16973">paper</a>] [<a href="https://github.com/PRIS-CV/DemoFusion">code</a>]  [<a href="https://huggingface.co/spaces/radames/Enhance-This-DemoFusion-SDXL">demo</a>] [<a href="https://colab.research.google.com/github/camenduru/DemoFusion-colab/blob/main/DemoFusion_colab.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Image and video synthesis and generation
    <br/>
    <strong>Session:</strong> Wed 19 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #132
</p>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/b0833f6b-6924-4f28-b409-ae85aaaa4dd6" title="DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/2a0219f5-9f1e-47e1-a968-d4d98154feb2" alt="DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2306.14435" title="DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing">
        <strong>üî• DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</strong>
    </a>
    <br/>
    Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai
    <br/>
    [<a href="https://arxiv.org/abs/2306.14435">paper</a>] [<a href="https://github.com/Yujun-Shi/DragDiffusion">code</a>] [<a href="https://youtu.be/rysOFTpDBhc">video</a>]  
    <br/>
    <strong>Topic:</strong> Image and video synthesis and generation
    <br/>
    <strong>Session:</strong> Wed 19 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #392
</p>
<br/>
<br/>

### low-level vision

<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/8eb6b4f0-4ae6-4615-9921-f73fa2aa3766" title="XFeat: Accelerated Features for Lightweight Image Matching">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/50b6d16f-c2d8-49a4-8c15-a31d6f9a3c44" alt="XFeat: Accelerated Features for Lightweight Image Matching" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2404.19174" title="XFeat: Accelerated Features for Lightweight Image Matching">
        <strong>XFeat: Accelerated Features for Lightweight Image Matching</strong>
    </a>
    <br/>
    Guilherme Potje, Felipe Cadar, Andre Araujo, Renato Martins, Erickson R. Nascimento
    <br/>
    [<a href="https://arxiv.org/abs/2404.19174">paper</a>] [<a href="https://github.com/verlab/accelerated_features">code</a>] [<a href="https://youtu.be/RamC70IkZuI">video</a>] [<a href="https://huggingface.co/spaces/qubvel-hf/xfeat">demo</a>] [<a href="https://colab.research.google.com/github/verlab/accelerated_features/blob/main/notebooks/xfeat_matching.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Low-level vision
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #245
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/038bef8f-a6df-440d-9ebc-b58f69beb338" title="Robust Image Denoising through Adversarial Frequency Mixup">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/03cc753c-f875-479e-bca2-e0375e9929a6" alt="Robust Image Denoising through Adversarial Frequency Mixup" width="400px" align="left" />
    </a>
    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ryou_Robust_Image_Denoising_through_Adversarial_Frequency_Mixup_CVPR_2024_paper.html" title="Robust Image Denoising through Adversarial Frequency Mixup">
        <strong>Robust Image Denoising through Adversarial Frequency Mixup</strong>
    </a>
    <br/>
    Donghun Ryou, Inju Ha, Hyewon Yoo, Dongwan Kim, Bohyung Han
    <br/>
    [<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ryou_Robust_Image_Denoising_through_Adversarial_Frequency_Mixup_CVPR_2024_paper.html">paper</a>] [<a href="https://github.com/dhryougit/AFM">code</a>] [<a href="https://youtu.be/zQ0pwFSk7uo">video</a>]  
    <br/>
    <strong>Topic:</strong> Low-level vision
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #250
</p>
<br/>
<br/>

### multi-modal learning

<p align="left">
    <a href="https://arxiv.org/abs/2310.03744" title="Improved Baselines with Visual Instruction Tuning">
        <strong>üî• Improved Baselines with Visual Instruction Tuning</strong>
    </a>
    <br/>
    Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee
    <br/>
    [<a href="https://arxiv.org/abs/2310.03744">paper</a>] [<a href="https://github.com/LLaVA-VL/LLaVA-NeXT">code</a>]   
    <br/>
    <strong>Topic:</strong> Multi-modal learning
    <br/>
    <strong>Session:</strong> Fri 21 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #209
</p>
<br/>

### recognition: categorization, detection, retrieval

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31301.png?t=1717420504.9897285" title="DETRs Beat YOLOs on Real-time Object Detection">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/3732bfdd-4be4-45cd-8353-e056094f9fec" alt="DETRs Beat YOLOs on Real-time Object Detection" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2304.08069" title="DETRs Beat YOLOs on Real-time Object Detection">
        <strong>DETRs Beat YOLOs on Real-time Object Detection</strong>
    </a>
    <br/>
    Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, Jie Chen
    <br/>
    [<a href="https://arxiv.org/abs/2304.08069">paper</a>] [<a href="https://github.com/lyuwenyu/RT-DETR">code</a>] [<a href="https://www.youtube.com/watch?v=UOc0qMSX4Ac">video</a>]  
    <br/>
    <strong>Topic:</strong> Recognition: Categorization, detection, retrieval
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #229
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/f9023a28-aca5-4965-a194-984c62348dc0" title="YOLO-World: Real-Time Open-Vocabulary Object Detection">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/b9f0bb1e-91d4-4ea3-83c6-ee0817afc1bf" alt="YOLO-World: Real-Time Open-Vocabulary Object Detection" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2401.17270" title="YOLO-World: Real-Time Open-Vocabulary Object Detection">
        <strong>YOLO-World: Real-Time Open-Vocabulary Object Detection</strong>
    </a>
    <br/>
    Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, Ying Shan
    <br/>
    [<a href="https://arxiv.org/abs/2401.17270">paper</a>] [<a href="https://github.com/AILab-CVC/YOLO-World">code</a>] [<a href="https://youtu.be/X7gKBGVz4vs">video</a>] [<a href="https://huggingface.co/spaces/SkalskiP/YOLO-World">demo</a>] [<a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Recognition: Categorization, detection, retrieval
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #223
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31732.png?t=1717298372.5822952" title="Object Recognition as Next Token Prediction">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31732.png?t=1717298372.5822952" alt="Object Recognition as Next Token Prediction" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.02142" title="Object Recognition as Next Token Prediction">
        <strong>üî• Object Recognition as Next Token Prediction</strong>
    </a>
    <br/>
    Kaiyu Yue, Bor-Chun Chen, Jonas Geiping, Hengduo Li, Tom Goldstein, Ser-Nam Lim
    <br/>
    [<a href="https://arxiv.org/abs/2312.02142">paper</a>] [<a href="https://github.com/kaiyuyue/nxtp">code</a>] [<a href="https://youtu.be/xeI8dZIpoco">video</a>]  [<a href="https://colab.research.google.com/drive/1pJX37LP5xGLDzD3H7ztTmpq1RrIBeWX3?usp=sharing">colab</a>]
    <br/>
    <strong>Topic:</strong> Recognition: Categorization, detection, retrieval
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #199
</p>
<br/>
<br/>

### segmentation, grouping and shape analysis

<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/62d34981-73d6-49b2-8058-46ec99bac94d" title="RobustSAM: Segment Anything Robustly on Degraded Images">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/ee15d3bc-c391-44f9-b35b-24af714ef119" alt="RobustSAM: Segment Anything Robustly on Degraded Images" width="400px" align="left" />
    </a>
    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_RobustSAM_Segment_Anything_Robustly_on_Degraded_Images_CVPR_2024_paper.html" title="RobustSAM: Segment Anything Robustly on Degraded Images">
        <strong>üî• RobustSAM: Segment Anything Robustly on Degraded Images</strong>
    </a>
    <br/>
    Wei-Ting Chen, Yu-Jiet Vong, Sy-Yen Kuo, Sizhou Ma, Jian Wang
    <br/>
    [<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Chen_RobustSAM_Segment_Anything_Robustly_on_Degraded_Images_CVPR_2024_paper.html">paper</a>]  [<a href="https://www.youtube.com/watch?v=Awukqkbs6zM">video</a>]  
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #378
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30253.png?t=1716781257.513028" title="Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/0c43b789-f2e8-4ff9-ae46-b5a87de1b921" alt="Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation" width="400px" align="left" />
    </a>
    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Frozen_CLIP_A_Strong_Backbone_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2024_paper.html" title="Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation">
        <strong>üî• Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation</strong>
    </a>
    <br/>
    Bingfeng Zhang, Siyue Yu, Yunchao Wei, Yao Zhao, Jimin Xiao
    <br/>
    [<a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Frozen_CLIP_A_Strong_Backbone_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2024_paper.html">paper</a>] [<a href="https://github.com/zbf1991/WeCLIP">code</a>] [<a href="https://youtu.be/Lh489nTm_M0">video</a>]  
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #351
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/2f2bf794-3981-48c8-992d-04dd32ee9ced" title="Semantic-aware SAM for Point-Prompted Instance Segmentation">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/f1ed2755-1df1-45fe-810b-5fc98b4b52e1" alt="Semantic-aware SAM for Point-Prompted Instance Segmentation" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.15895" title="Semantic-aware SAM for Point-Prompted Instance Segmentation">
        <strong>üî• Semantic-aware SAM for Point-Prompted Instance Segmentation</strong>
    </a>
    <br/>
    Zhaoyang Wei, Pengfei Chen, Xuehui Yu, Guorong Li, Jianbin Jiao, Zhenjun Han
    <br/>
    [<a href="https://arxiv.org/abs/2312.15895">paper</a>] [<a href="https://github.com/zhaoyangwei123/SAPNet">code</a>] [<a href="https://youtu.be/42-tJFmT7Ao">video</a>]  
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #331
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2403.15789" title="In-Context Matting">
        <strong>üî• In-Context Matting</strong>
    </a>
    <br/>
    He Guo, Zixuan Ye, Zhiguo Cao, Hao Lu
    <br/>
    [<a href="https://arxiv.org/abs/2403.15789">paper</a>] [<a href="https://github.com/tiny-smart/in-context-matting">code</a>]   
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #343
</p>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/bfe79038-706d-491b-ac99-083f421dc5ec" title="General Object Foundation Model for Images and Videos at Scale">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/4f0ed38d-28aa-4766-b290-940cbc6711d6" alt="General Object Foundation Model for Images and Videos at Scale" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.09158" title="General Object Foundation Model for Images and Videos at Scale">
        <strong>üî• General Object Foundation Model for Images and Videos at Scale</strong>
    </a>
    <br/>
    Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, Song Bai
    <br/>
    [<a href="https://arxiv.org/abs/2312.09158">paper</a>] [<a href="https://github.com/FoundationVision/GLEE">code</a>] [<a href="https://www.youtube.com/watch?v=PSVhfTPx0GQ">video</a>]  
    <br/>
    <strong>Topic:</strong> Segmentation, grouping and shape analysis
    <br/>
    <strong>Session:</strong> Wed 19 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #350
</p>
<br/>
<br/>

### self-supervised or unsupervised representation learning

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30014.png?t=1717339970.9614518" title="InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30014.png?t=1717339970.9614518" alt="InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.14238" title="InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks">
        <strong>üî• InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</strong>
    </a>
    <br/>
    Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai
    <br/>
    [<a href="https://arxiv.org/abs/2312.14238">paper</a>] [<a href="https://github.com/OpenGVLab/InternVL">code</a>]  [<a href="https://huggingface.co/spaces/OpenGVLab/InternVL">demo</a>] 
    <br/>
    <strong>Topic:</strong> Self-supervised or unsupervised representation learning
    <br/>
    <strong>Session:</strong> Fri 21 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #412
</p>
<br/>
<br/>

### video: low-level analysis, motion, and tracking

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/29590.png?t=1717456006.3308516" title="Matching Anything by Segmenting Anything">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/bb451f47-ba3e-4e34-a7c0-3410b64d9339" alt="Matching Anything by Segmenting Anything" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2406.04221" title="Matching Anything by Segmenting Anything">
        <strong>üî• Matching Anything by Segmenting Anything</strong>
    </a>
    <br/>
    Siyuan Li, Lei Ke, Martin Danelljan, Luigi Piccinelli, Mattia Segu, Luc Van Gool, Fisher Yu
    <br/>
    [<a href="https://arxiv.org/abs/2406.04221">paper</a>] [<a href="https://github.com/siyuanliii/masa">code</a>] [<a href="https://youtu.be/KDQVujKAWFQ">video</a>]  
    <br/>
    <strong>Topic:</strong> Video: Low-level analysis, motion, and tracking
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #421
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/9711186c-b05b-472d-b095-d98dbe386171" title="DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/18caf2db-5dab-4251-9eeb-e2397c67eb3f" alt="DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2403.02075" title="DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction">
        <strong>DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction</strong>
    </a>
    <br/>
    Weiyi Lv, Yuhang Huang, Ning Zhang, Ruei-Sung Lin, Mei Han, Dan Zeng
    <br/>
    [<a href="https://arxiv.org/abs/2403.02075">paper</a>] [<a href="https://github.com/Kroery/DiffMOT">code</a>]   
    <br/>
    <strong>Topic:</strong> Video: Low-level analysis, motion, and tracking
    <br/>
    <strong>Session:</strong> Thu 20 Jun 8 p.m. EDT ‚Äî 9:30 p.m. EDT #455
</p>
<br/>
<br/>

### vision, language, and reasoning

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31492.png?t=1717327133.6073072" title="Alpha-CLIP: A CLIP Model Focusing on Wherever You Want">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/4480d88a-7f8f-48c2-bcb0-bde3b694dfd8" alt="Alpha-CLIP: A CLIP Model Focusing on Wherever You Want" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.03818" title="Alpha-CLIP: A CLIP Model Focusing on Wherever You Want">
        <strong>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</strong>
    </a>
    <br/>
    Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang
    <br/>
    [<a href="https://arxiv.org/abs/2312.03818">paper</a>] [<a href="https://github.com/SunzeY/AlphaCLIP">code</a>] [<a href="https://youtu.be/QCEIKPZpZz0">video</a>] [<a href="https://huggingface.co/spaces/Zery/Alpha-CLIP_LLaVA-1.5">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #327
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2401.06209" title="Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs">
        <strong>üî• Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</strong>
    </a>
    <br/>
    Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining Xie
    <br/>
    [<a href="https://arxiv.org/abs/2401.06209">paper</a>] [<a href="https://github.com/tsb0601/MMVP">code</a>]   
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #390
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30109.png?t=1717509456.89997" title="LISA: Reasoning Segmentation via Large Language Model">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/fc2699d9-7bd2-4c3a-8e6c-4961505cc802" alt="LISA: Reasoning Segmentation via Large Language Model" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2308.00692" title="LISA: Reasoning Segmentation via Large Language Model">
        <strong>üî• LISA: Reasoning Segmentation via Large Language Model</strong>
    </a>
    <br/>
    Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia
    <br/>
    [<a href="https://arxiv.org/abs/2308.00692">paper</a>] [<a href="https://github.com/dvlab-research/LISA">code</a>]  [<a href="http://103.170.5.190:7870/">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #413
</p>
<br/>
<br/>


<p align="left">
    <a href="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/53e03a08-4dd9-451a-975e-e3654fa5bc71" title="ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts">
        <img src="https://github.com/SkalskiP/top-cvpr-2024-papers/assets/26109316/6d1536ae-3f96-49d9-a05f-9648b925cdb5" alt="ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2312.00784" title="ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts">
        <strong>ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts</strong>
    </a>
    <br/>
    Mu Cai, Haotian Liu, Dennis Park, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Yong Jae Lee
    <br/>
    [<a href="https://arxiv.org/abs/2312.00784">paper</a>] [<a href="https://github.com/WisconsinAIVision/ViP-LLaVA">code</a>] [<a href="https://youtu.be/j_l1bRQouzc">video</a>] [<a href="https://pages.cs.wisc.edu/~mucai/vip-llava.html">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #317
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31040.png?t=1718300473.5736258" title="MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/31040.png?t=1718300473.5736258" alt="MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2311.16502" title="MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI">
        <strong>üî• MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI</strong>
    </a>
    <br/>
    Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen
    <br/>
    [<a href="https://arxiv.org/abs/2311.16502">paper</a>]    
    <br/>
    <strong>Topic:</strong> Vision, language, and reasoning
    <br/>
    <strong>Session:</strong> Thu 20 Jun 1:30 p.m. EDT ‚Äî 3 p.m. EDT #382
</p>
<br/>
<br/>

<!--- AUTOGENERATED_PAPERS_LIST -->

## ü¶∏ contribution

We would love your help in making this repository even better! If you know of an amazing
paper that isn't listed here, or if you have any suggestions for improvement, feel free
to open an
[issue](https://github.com/SkalskiP/top-cvpr-2024-papers/issues)
or submit a
[pull request](https://github.com/SkalskiP/top-cvpr-2024-papers/pulls).
